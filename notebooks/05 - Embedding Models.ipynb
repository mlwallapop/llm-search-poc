{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Embedding Models\n",
    "\n",
    "Imagine turning any piece of text‚Äîtweets, docs, or books‚Äîinto a compact, machine-readable form. That‚Äôs what **embedding models** do! They transform text into numerical vectors that capture meaning, enabling semantic search, ranking, and clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Concepts\n",
    "\n",
    "![Embedding Models](assets/embeddings_concept-975a9aaba52de05b457a1aeff9a7393a.png \"Embedding Models\")\n",
    "\n",
    "1. **Embed text** ‚û°Ô∏è Convert text into a vector (a list of numbers)\n",
    "2. **Compare meaning** ‚û°Ô∏è Use math (like cosine similarity) to compare vectors\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è A Quick History\n",
    "\n",
    "- üìå **BERT (2018)**: Google‚Äôs breakthrough for understanding text  \n",
    "- ‚ö° **SBERT**: Tuned for sentence embeddings with better speed & accuracy  \n",
    "- üß™ **MTEB**: A benchmark to compare modern embedding models\n",
    "\n",
    "**Explore more:**\n",
    "- [BERT Paper](#)\n",
    "- [Cameron Wolfe's Review](#)\n",
    "- [MTEB Leaderboard](#)\n",
    "\n",
    "---\n",
    "\n",
    "## üß© LangChain Interface\n",
    "\n",
    "LangChain makes working with embeddings simple:\n",
    "\n",
    "- `embed_documents` ‚Üí for multiple texts  \n",
    "- `embed_query` ‚Üí for a single query  \n",
    "\n",
    "üõ†Ô∏è **Batch embed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ†Ô∏è **Query embed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more:** [Embedding Integrations](#) ¬∑ [How-to Guides](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìè Measuring Similarity\n",
    "\n",
    "Embeddings are like coordinates in space. The closer two vectors are, the more semantically related their texts are.\n",
    "\n",
    "### Common Similarity Metrics:\n",
    "\n",
    "- üìê **Cosine Similarity**: Angle between vectors  \n",
    "- üìç **Euclidean Distance**: Straight-line distance  \n",
    "- üéØ **Dot Product**: Projection of one onto another  \n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)\n",
    "```\n",
    "\n",
    "**Suggested by:** OpenAI (for their models)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- [Simon Willison on Embeddings](https://simonwillison.net/2023/Oct/23/embeddings/)  \n",
    "- [Google on Similarity Metrics](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)  \n",
    "- [OpenAI FAQ on Similarity](https://platform.openai.com/docs/guides/embeddings/faq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
