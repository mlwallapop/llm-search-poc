{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Embedding Models\n",
    "\n",
    "Imagine turning any piece of textâ€”tweets, docs, or booksâ€”into a compact, machine-readable form. Thatâ€™s what **embedding models** do! They transform text into numerical vectors that capture meaning, enabling semantic search, ranking, and clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Key Concepts\n",
    "\n",
    "![Embedding Models](assets/embeddings_concept-975a9aaba52de05b457a1aeff9a7393a.png \"Embedding Models\")\n",
    "\n",
    "1. **Embed text** â¡ï¸ Convert text into a vector (a list of numbers)\n",
    "2. **Compare meaning** â¡ï¸ Use math (like cosine similarity) to compare vectors\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ•°ï¸ A Quick History\n",
    "\n",
    "- ğŸ“Œ **BERT (2018)**: Googleâ€™s breakthrough for understanding text  \n",
    "- âš¡ **SBERT**: Tuned for sentence embeddings with better speed & accuracy  \n",
    "- ğŸ§ª **MTEB**: A benchmark to compare modern embedding models\n",
    "\n",
    "**Explore more:**\n",
    "- [BERT Paper](#)\n",
    "- [Cameron Wolfe's Review](#)\n",
    "- [MTEB Leaderboard](#)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© LangChain Interface\n",
    "\n",
    "LangChain makes working with embeddings simple:\n",
    "\n",
    "- `embed_documents` â†’ for multiple texts  \n",
    "- `embed_query` â†’ for a single query  \n",
    "\n",
    "ğŸ› ï¸ **Example Placeholder:** _Embed a list of strings_  \n",
    "ğŸ”— Replace with your code snippet: `<<EMBED_DOCUMENTS_EXAMPLE>>`\n",
    "\n",
    "ğŸ› ï¸ **Example Placeholder:** _Embed a query_  \n",
    "ğŸ”— Replace with your code snippet: `<<EMBED_QUERY_EXAMPLE>>`\n",
    "\n",
    "**Learn more:** [Embedding Integrations](#) Â· [How-to Guides](#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ“ Measuring Similarity\n",
    "\n",
    "Embeddings are like coordinates in space. The closer two vectors are, the more semantically related their texts are.\n",
    "\n",
    "### Common Similarity Metrics:\n",
    "\n",
    "- ğŸ“ **Cosine Similarity**: Angle between vectors  \n",
    "- ğŸ“ **Euclidean Distance**: Straight-line distance  \n",
    "- ğŸ¯ **Dot Product**: Projection of one onto another  \n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)\n",
    "```\n",
    "\n",
    "**Suggested by:** OpenAI (for their models)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Further Reading\n",
    "\n",
    "- [Simon Willison on Embeddings](https://simonwillison.net/2023/Oct/23/embeddings/)  \n",
    "- [Google on Similarity Metrics](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)  \n",
    "- [OpenAI FAQ on Similarity](https://platform.openai.com/docs/guides/embeddings/faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
