{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "from langchain_community.agent_toolkits.github.toolkit import GitHubToolkit\n",
    "from langchain_community.utilities.github import GitHubAPIWrapper\n",
    "\n",
    "github = GitHubAPIWrapper()\n",
    "toolkit = GitHubToolkit.from_github_api_wrapper(github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Issues\n",
      "Get Issue\n",
      "Comment on Issue\n",
      "List open pull requests (PRs)\n",
      "Get Pull Request\n",
      "Overview of files included in PR\n",
      "Create Pull Request\n",
      "List Pull Requests' Files\n",
      "Create File\n",
      "Read File\n",
      "Update File\n",
      "Delete File\n",
      "Overview of existing files in Main branch\n",
      "Overview of files in current working branch\n",
      "List branches in this repository\n",
      "Set active branch\n",
      "Create a new branch\n",
      "Get files from a directory\n",
      "Search issues and pull requests\n",
      "Search code\n",
      "Create review request\n"
     ]
    }
   ],
   "source": [
    "tools = toolkit.get_tools()\n",
    "for tool in tools:\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can i run the project locally? Can you tell me what the readme says?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_read_file (call_04G1akRJYxkGtZiHQuCFFyad)\n",
      " Call ID: call_04G1akRJYxkGtZiHQuCFFyad\n",
      "  Args:\n",
      "    formatted_filepath: README.md\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_read_file\n",
      "\n",
      "# LLM-Powered Search PoC\n",
      "\n",
      "This repository contains a proof-of-concept (PoC) application that demonstrates how to use a Large Language Model (LLM) to improve search relevance and re-rank search results. The project supports both manual query analysis and bulk analysis from a CSV file. It also includes a settings panel where you can configure the prompt templates and LLM parameters (provider, model, temperature).\n",
      "\n",
      "Repository: https://github.com/mlwallapop/llm-search-poc\n",
      "\n",
      "## Functionalities\n",
      "\n",
      "- **Manual Query Analysis:**  \n",
      "  Type a query (e.g., \"silla de madera para mesa de exterior\") and retrieve search results from the Wallapop API.  \n",
      "  The app shows three views:\n",
      "  - **Baseline:** The original order from the search engine.\n",
      "  - **LLM Listwise Ranking:** Re-ranks results using a listwise approach with detailed reasoning.\n",
      "  - **LLM Pointwise Ranking:** Re-ranks results using individual scores for each result.\n",
      "\n",
      "- **CSV Bulk Analysis:**  \n",
      "  Upload a CSV file containing search keywords and associated metrics.  \n",
      "  All numeric columns are automatically detected and range sliders are displayed (grouped four per row) in an \"Advanced Filters\" panel.  \n",
      "  Once you’re happy with the filtered data, click the \"Process Bulk\" button to analyze each keyword from the filtered CSV. Each keyword’s analysis is displayed in a collapsible panel with sub-tabs for Baseline, Listwise, and Pointwise ranking.\n",
      "\n",
      "- **Settings:**  \n",
      "  Under the Settings tab you can:\n",
      "  - Configure the prompt templates used for pointwise and listwise ranking.\n",
      "  - Select the LLM provider. Only providers with corresponding API keys set in your environment (e.g., OpenAI, Gemini, or Bedrock) are shown.\n",
      "  - Choose an appropriate model and adjust the temperature.\n",
      "  \n",
      "  Changes are saved to the configuration and immediately affect how the ranking functions generate prompts.\n",
      "\n",
      "## Setup and Installation\n",
      "\n",
      "1. **Clone the Repository:**\n",
      "\n",
      "```bash\n",
      "git clone https://github.com/mlwallapop/llm-search-poc.git\n",
      "cd llm-search-poc\n",
      "```\n",
      "\n",
      "2. **Create a Virtual Environment and Install Dependencies: ONLY IF USING LOCAL ENVIRONMENT**\n",
      "\n",
      "```bash\n",
      "python3.11 -m venv venv\n",
      "source venv/bin/activate        # On Windows use: venv\\Scripts\\activate\n",
      "pip install --upgrade pip\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "3. **Set Up Environment Variables:**\n",
      "\n",
      "Copy the provided env.dist file to .env and update with your API keys:\n",
      "\n",
      "```bash\n",
      "cp env.dist .env\n",
      "```\n",
      "\n",
      "Edit the .env file and fill in your:\n",
      "- OPENAI_API_KEY\n",
      "- GEMINI_API_KEY\n",
      "- BEDROCK_API_KEY\n",
      "\n",
      "## Running the Project\n",
      "\n",
      "### Running with Docker Compose (for development)\n",
      "\n",
      "A Docker Compose configuration is provided to support hot reload during development.\n",
      "\n",
      "1. **Build and Run:**\n",
      "\n",
      "```bash\n",
      "docker-compose up --build\n",
      "```\n",
      "\n",
      "Your app will be accessible at [http://localhost:7777](http://localhost:7777).\n",
      "\n",
      "### Running Locally\n",
      "\n",
      "To run the application locally, execute:\n",
      "\n",
      "```bash\n",
      "streamlit run app/streamlit_app.py\n",
      "```\n",
      "\n",
      "The app will launch in your browser using a full-screen wide layout. It provides top-level tabs for Manual Query, CSV Bulk Analysis, and Settings.\n",
      "\n",
      "## Explanation of Settings\n",
      "\n",
      "- **Prompt Templates:**  \n",
      "  In the Settings tab you can edit:\n",
      "  - **Pointwise Prompt Template:** Used to score individual query-document pairs.\n",
      "  - **Listwise Prompt Template:** Used to rank a set of search results along with detailed reasoning and query interpretation.\n",
      "  \n",
      "- **LLM Settings:**  \n",
      "  In the Settings tab you can configure:\n",
      "  - **LLM Provider:** The available providers are shown based on which API keys are set in your environment. Options include ChatOpenAI, ChatGemini, and ChatBedrock.\n",
      "  - **LLM Model:** Choose from models appropriate for the selected provider (e.g., \"gpt-4o-mini\" for ChatOpenAI, \"gemini-1.5-pro\" for ChatGemini, \"llama3.3:latest\" for ChatBedrock).\n",
      "  - **LLM Temperature:** Adjusts the randomness of the responses (from 0 to 1).\n",
      "\n",
      "When you click \"Save Settings,\" your selections update the configuration used by the ranking functions.\n",
      "\n",
      "## Project Structure\n",
      "\n",
      "- **app/**: Contains the Streamlit application (streamlit_app.py).\n",
      "- **src/**: Contains the core logic:\n",
      "  - **search_engine.py:** Code to query the Wallapop API.\n",
      "  - **ranking.py:** Functions to compute LLM-based scores and rankings.\n",
      "  - **schemas.py:** Pydantic models for structured LLM responses.\n",
      "  - **metrics.py:** Metric functions (e.g., NDCG calculation).\n",
      "  - **config.py:** Default configuration for prompt templates and LLM settings.\n",
      "- **.env:** Environment variables file (copy from env.dist).\n",
      "- **docker-compose.yml & Dockerfile:** For running the app in a container with hot reload during development.\n",
      "- **requirements.txt:** Python dependencies list.\n",
      "\n",
      "## Additional Notes\n",
      "\n",
      "- This project is a proof-of-concept designed for demonstration and learning purposes.\n",
      "- The UI is organized into top-level tabs to separate Manual Query, CSV Bulk Analysis, and Settings.\n",
      "- For CSV Bulk Analysis, each keyword’s analysis appears in a collapsible panel with sub-tabs to keep the interface clean.\n",
      "- Enjoy exploring and tweaking the LLM-Powered Search PoC!\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The README file provides comprehensive instructions on how to run the project locally. Here's a summary of the relevant sections:\n",
      "\n",
      "### Setup and Installation\n",
      "\n",
      "1. **Clone the Repository:**\n",
      "   ```bash\n",
      "   git clone https://github.com/mlwallapop/llm-search-poc.git\n",
      "   cd llm-search-poc\n",
      "   ```\n",
      "\n",
      "2. **Create a Virtual Environment and Install Dependencies (only if using a local environment):**\n",
      "   ```bash\n",
      "   python3.11 -m venv venv\n",
      "   source venv/bin/activate        # On Windows use: venv\\Scripts\\activate\n",
      "   pip install --upgrade pip\n",
      "   pip install -r requirements.txt\n",
      "   ```\n",
      "\n",
      "3. **Set Up Environment Variables:**\n",
      "   Copy the provided `env.dist` file to `.env` and update it with your API keys:\n",
      "   ```bash\n",
      "   cp env.dist .env\n",
      "   ```\n",
      "   Edit the `.env` file to fill in your:\n",
      "   - `OPENAI_API_KEY`\n",
      "   - `GEMINI_API_KEY`\n",
      "   - `BEDROCK_API_KEY`\n",
      "\n",
      "### Running the Project\n",
      "\n",
      "#### Running Locally\n",
      "To run the application locally, execute:\n",
      "```bash\n",
      "streamlit run app/streamlit_app.py\n",
      "```\n",
      "The app will launch in your browser using a full-screen wide layout.\n",
      "\n",
      "### Additional Information\n",
      "- The application is a proof-of-concept that utilizes a Large Language Model (LLM) to enhance search results.\n",
      "- It supports manual query analysis and bulk analysis from a CSV file.\n",
      "- The settings panel allows configuration of prompt templates and LLM parameters.\n",
      "\n",
      "For more details, you can refer to the full README content.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Select example tool\n",
    "model_tools = [tool for tool in toolkit.get_tools() if tool.name == \"Read File\"]\n",
    "assert len(model_tools) == 1\n",
    "model_tools[0].name = \"get_read_file\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "agent_executor = create_react_agent(llm, model_tools)\n",
    "\n",
    "example_query = \"How can i run the project locally? Can you tell me what the readme says?\"\n",
    "\n",
    "events = agent_executor.stream(\n",
    "    {\"messages\": [(\"user\", example_query)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
