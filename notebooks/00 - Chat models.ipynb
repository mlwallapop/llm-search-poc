{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat models\n",
    "\n",
    "LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Parameters\n",
    "\n",
    "Many chat models have standardized parameters that can be used to configure the model:\n",
    "\n",
    "| **Parameter**   | **Description** |\n",
    "|-----------------|-----------------|\n",
    "| `model`         | The name or identifier of the specific AI model you want to use (e.g., `\"gpt-3.5-turbo\"` or `\"gpt-4\"`). |\n",
    "| `temperature`   | Controls the randomness of the model's output. A higher value (e.g., `1.0`) makes responses more creative, while a lower value (e.g., `0.0`) makes them more deterministic and focused. |\n",
    "| `timeout`       | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely. |\n",
    "| `max_tokens`    | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be. |\n",
    "| `stop`          | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response. |\n",
    "| `max_retries`   | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. |\n",
    "| `api_key`       | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model. |\n",
    "| `base_url`      | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests. |\n",
    "| `rate_limiter`  | An optional `BaseRateLimiter` to space out requests to avoid exceeding rate limits. See rate-limiting below for more details. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 13, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BEmmK799zYy0EurV9lWFfF5HEtO9h', 'finish_reason': 'stop', 'logprobs': None} id='run-2ae2725a-d9db-4f01-96ef-0f03075f6a44-0' usage_metadata={'input_tokens': 13, 'output_tokens': 32, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "result = model.invoke('Hi! How are you?')\n",
    "print(result)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    "\n",
    "Each message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\n",
    "\n",
    "LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "\n",
    "### What is inside a message?\n",
    "A message typically consists of the following pieces of information:\n",
    "\n",
    "- **Role**: The role of the message (e.g., \"user\", \"assistant\").\n",
    "- **Content**: The content of the message (e.g., text, multimodal data).\n",
    "- **Additional metadata**: id, name, token usage and other model-specific metadata.\n",
    "\n",
    "### Role\n",
    "\n",
    "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
    "\n",
    "| **Role**         | **Description** |\n",
    "|------------------|-----------------|\n",
    "| `system`         | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers. |\n",
    "| `user`           | Represents input from a user interacting with the model, usually in the form of text or other interactive input. |\n",
    "| `assistant`      | Represents a response from the model, which can include text or a request to invoke tools. |\n",
    "| `tool`           | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling. |\n",
    "| `function (legacy)` | This is a legacy role, corresponding to OpenAI's legacy function-calling API. `tool` role should be used instead. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanMessage\n",
    "The HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 13, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BEmnKPB7KoWfw9ZFgBNMZrrS4m5Ib', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec6b2027-ac90-44c1-be9d-52a7262c59d5-0', usage_metadata={'input_tokens': 13, 'output_tokens': 32, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BEmnL8qhmNtldsaKAItz5sMvrvbhS', 'finish_reason': 'stop', 'logprobs': None}, id='run-acbb3db4-721a-48c5-a09d-20002f052c79-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "ai_message = model.invoke([HumanMessage(\"Tell me a joke\")])\n",
    "ai_message # <-- AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIMessage\n",
    "**AIMessage** is used to represent a message with the role **assistant**: this is the response from the model, which can include text or a request to invoke tools.\n",
    "\n",
    "#### AIMessage Attributes\n",
    "\n",
    "An `AIMessage` has the following attributes. The attributes marked as **Standardized** are ones that LangChain attempts to unify across different chat model providers. **Raw** fields are specific to the model provider and may vary.\n",
    "\n",
    "| **Attribute**         | **Standardized/Raw** | **Description** |\n",
    "|------------------------|----------------------|-----------------|\n",
    "| `content`              | Raw                  | Usually a string, but can be a list of content blocks. See content for details. |\n",
    "| `tool_calls`           | Standardized         | Tool calls associated with the message. See tool calling for details. |\n",
    "| `invalid_tool_calls`   | Standardized         | Tool calls with parsing errors associated with the message. See tool calling for details. |\n",
    "| `usage_metadata`       | Standardized         | Usage metadata for a message, such as token counts. See Usage Metadata API Reference. |\n",
    "| `id`                   | Standardized         | An optional unique identifier for the message, ideally provided by the provider/model that created the message. |\n",
    "| `response_metadata`    | Raw                  | Response metadata, e.g., response headers, logprobs, token counts. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BEmnPutG9QDCbaqAqM3E5PRxrNUEL', 'finish_reason': 'stop', 'logprobs': None}, id='run-2cb510ad-a117-4adb-a006-bbf0bc1fac11-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "ai_message = model.invoke([HumanMessage(\"Tell me a joke\")])\n",
    "ai_message # <-- AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIMessageChunk\n",
    "It is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\n",
    "\n",
    "It is returned from the `stream`, `astream` and `astream_events` methods of the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' color' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sky' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' vary' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' depending' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' on' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' time' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' day' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' weather' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' conditions' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' During' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' day' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sky' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' typically' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' blue' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' but' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' it' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' also' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' appear' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' gray' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' pink' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' orange' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' or' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' red' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' during' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sunrise' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sunset' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' At' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' night' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sky' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' appears' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' black' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' with' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' stars' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' sometimes' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' moon' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content=' visible' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream([HumanMessage(\"what color is the sky?\")]):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating\n",
    "`AIMessageChunks` support the `+` operator to merge them into a single `AIMessage`. This is useful when you want to display the final response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The color of the sky' additional_kwargs={} response_metadata={} id='run-d95651aa-0efa-4150-ab6c-614db45f3545'\n"
     ]
    }
   ],
   "source": [
    "ai_message = chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4] + chunks[5]\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToolMessage\n",
    "This represents a message with role `tool`, which contains the result of calling a tool.\n",
    "We'll check it out later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
